{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural networks\n",
    "\n",
    "A convolutional neural network (CNN or Convnet) is a class of neural networks, a machine learning algorithm. CNN's are especially good at visual data analysis, such as image classification; by detecting patterins, mostly in the form of shapes (edges, circles, etc) and colours. Before CNN's, computer vision (CV) experts would develop filters manually to detect patterns, e.g. Sobel filter which detects edges. Nowadays with deep learning, specifically with convolutional layers, one can create these filters automatically by training a (convolutional) neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers\n",
    "\n",
    "CNN's differentiate themselves from 'regular' neural networks with special hidden layers called convolutional layers, putting the convolution in convolutional neural network. Convolutional layers transform the data by performing cross-correlations, also called convolving. But first let's take a high level view of the entire process.\n",
    "\n",
    "The way convolutional neural networks processrd images is in the form of a matrix. Each row and column representing an individual pixel of said image. You're probably familiar with the hello-world tutorial of neural networks by using a classic NN to classify handwritten digits. For the input layers you used every pixel as input neuron, it's not that different in CNN's except it's in the form of a matrix instead of a (flattened) vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![vvg16](sources/input-matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter\n",
    "\n",
    "Convolutional layer is the first layer that extract features from the images. Pixels are only related to its direct adjacent and close pixels. Convolving the image patternizes the relationship of more pixels across the image, larger related matrices. Convolution filtering detects patterns by looking at larger matrices of pixels while preserving relations and complexity. See animation below where a filter of 3x3 takes in 9 pixels and stores its output in a single output cell. The outcome matrix is also called a feature map.\n",
    "\n",
    "![cnn filter animation](sources/cnn-filter.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering above's animation, we still dont know what the output is. How does a filter consider the 9 input pixels and what is the resulting outcome? A convolutional filter is not just a width and height of how many pixels it should look at. The filter matrix itself has values too. Each positional value in the filter matrix is multiplied to the respected input position it's convolving.\n",
    "![convolution-filter](sources/convolution-filter.png)\n",
    "\n",
    "Letâ€™s suppose that we have four 3 x 3 filters for our first convolutional layer, and these filters are filled with the values you see below. These values can be represented visually by having -1s correspond to black, 1s correspond to white, and 0s correspond to grey. These 4 filters of 3x3 are detecting edges as depicted by the images, resulting in 4 feature maps.\n",
    "\n",
    "$$\\begin{bmatrix} -1 & 1 & 0 \\\\ -1 & 1 & 0 \\\\ -1 & 1 & 0 \\end{bmatrix}\n",
    "\\begin{bmatrix} -1 & -1 & -1 \\\\ 1 & 1 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix}\n",
    "\\begin{bmatrix} 0 & 1 & -1 \\\\ 0 & 1 & -1 \\\\ 0 & 1 & -1 \\end{bmatrix}\n",
    "\\begin{bmatrix} 0 & 0 & 0 \\\\ 1 & 1 & 1 \\\\ -1 & -1 & -1 \\end{bmatrix}$$\n",
    "\n",
    "![filter-sides](sources/filter-sides.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stride\n",
    "When computing the cross-correlation, we start with the convolution window at the top-left corner of the input matrix, and slide it over all possible locations from left to right, top to bottom. Above's animation showed a default `stride` of 1x1, also simpply called `no stride` (We must move at least 1 position/pixel or we're stuck forever), meaning that the filter shifts position by 1x1 pixel. This is an arbitrary configuration, one could set a stride of 2x3, resulting in the filter shifting 2 positions horizontally and 3 vertically every shift. Generally a stride of 1x1 is applied unless, for computational efficiency, one wants to downsample and thus skip intermediate positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "\n",
    "Consider above's animation, the input matrix is 7x7 and the output is 5x5, the output matrix is smaller than the input matrix. This is because the filter of 3x3 cannot fit on every position on the input matrix due to the borders. Convolving a 5x5 image with a 3x3 filter with a 1x1 `stride`, results in an output matrix of 3x3, a 64% decrease in complexity. \n",
    "\n",
    "In order to use convolutional filters without decreasing complexity one can configure a padding size. Padding adds an extra layer of pixels on each side of the image with a value of 0, allowing the filter to stride over the entire image.\n",
    "\n",
    "![same padding no strides](sources/cnn-all-filter.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max pooling\n",
    "\n",
    "Convolution layers are commonly followed by pooling layers to reduce the spatial size of the representation to reduce the parameter counts, reducing the computational complexity in following layers. Basically we select a pooling size to reduce the amount of the parameters by selecting the maximum, average, or sum values of the input matrix. Pooling layers also partially prevent overfitting as the specific relations are (literally) replaced by a larger overview. \n",
    "\n",
    "Pooling is applied for three reasons: To get local translational invariance, to get invariance against minor local changes and, most important, for data reduction.\n",
    "\n",
    "Applying a convolutional layer 2x2 filter on a handwritten digit would result in the following matrix. One can definitely notice a loss of detail in the digit but the pattern persists, while drastically reducing the amount of data to the next layer. Below depicts a max-pooling of 2x2, notice the maximum values in the coloured boxes on the left equals on the right.\n",
    "\n",
    "![filter-pool](sources/max-pool-seven3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1x1 layer (network in network)\n",
    "\n",
    "A filter of 1x1 at first glance sounds rather useless, multipling matrices to a matrix with a single row and column is just a normal multiplcation. Such as described below. \n",
    "\n",
    "$$\\begin{bmatrix} 3 & 5 & 2 \\\\ 2 & 6 & 4 \\\\ 1 & 3 & 4 \\end{bmatrix}\n",
    "\\begin{bmatrix} 2 \\end{bmatrix}\n",
    "\\begin{bmatrix} 6 & 10 & 4 \\\\ 4 & 12 & 8 \\\\ 2 & 6 & 8 \\end{bmatrix}$$\n",
    "\n",
    "However this was without taking channels into account, the feature maps depth. Let's refresh, max-pooling reduces the amount of parameters in the matrices, however, the amount of feature maps (depth) remain the same. In order to reduce the amount of feature maps we can introduce a 1x1 convolution layer, also called `Network in network`. A 1x1 convolution simply maps an input pixel with all it's channels to an output pixel, not relating at anything around itself. Often used to reduce the number of depth channels, as multipling volumes with (extremely) large depths is computationally time consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the code below. A convolutional neural network with 2 layers. One with a matrix input of 256x256 and 256 filters of 3x3, resulting in 512 feature maps after the very first first layer. In order to reduce the amount of feature maps (512) we can introduce a 1x1 convolutional layer. 64 filters of 1x1 resulting in exactly 64 feature maps with where the input depth values are mapped as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 256, 256, 512)     14336     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 256, 256, 64)      32832     \n",
      "=================================================================\n",
      "Total params: 47,168\n",
      "Trainable params: 47,168\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(512, (3,3), padding='same', activation='relu', input_shape=(256, 256, 3)))\n",
    "model.add(Conv2D(64, (1,1), activation='relu'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More advanced usages of 1x1 conv layers would be to promote learning across channels and in ineception architecture, far beyond the scope of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start off with a simple introduction into convolutional layers, eventually we'll move to complex architectures and compare results. Below's code should be interpreted more as pseudo code with further explanations, later on the best model to date will be created, trained, tested and applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import libraries...\n",
      "Imported all libraries!\n"
     ]
    }
   ],
   "source": [
    "print(\"Import libraries...\")\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "from mnist.loader import MNIST\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "print(\"Imported all libraries!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST dataset...\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading MNIST dataset...\")\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n",
      "model created!\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating model...\")\n",
    "# Creating model\n",
    "model = Sequential()\n",
    "# Conv2d layer with 64 filters of 3x3\n",
    "model.add(Conv2D(input_shape=(28,28,3),filters=64,kernel_size=(3,3), activation=\"relu\"))\n",
    "model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "# Max pooling layer, 2x2 stride\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "# Drop out, not covered in the notebook so far\n",
    "model.add(Dropout(0.25))\n",
    "# Flatten data as required for dense (fully connected) layer\n",
    "model.add(Flatten())\n",
    "# Classic NN layer with 1024 nodes\n",
    "model.add(Dense(units=1024,activation=\"relu\"))\n",
    "# Drop out, not covered in the notebook so far\n",
    "model.add(Dropout(0.5))\n",
    "# Final NN layer with 10 output nodes\n",
    "model.add(Dense(units=10, activation=\"softmax\"))\n",
    "print(\"model created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring model accuracy\n",
    "\n",
    "Generally machine learning models accuracy is measured in Mean Squared Error (MSE). However, for Neural Networks, it's generally more accurate to use `Cross-Entropy Error` (CSE) to measure accuracy. Suppose a neural network is classifying data and the outcome is a score from 0-1 how sure it is of each possible output label. To measure accuracy we consider  3 seperate inputs for the NN to predict; 2 were predicted correctly, and 1 was wrong. MSE will measure this as a 1/3 classification error, 2/3 correct which results in an 0.67 accuracy score. Whereas `Cross-Entropy error` (CSE) calculates how correct or wrong a given prediction is. E.g a prediction that was just barely right will receive a lower score in CSE than in MSE, same case when a prediction was only just incorrect, MSE sees it as entirely wrong whereas CSE still gives points for being almost right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 26, 26, 64)        1792      \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 26, 26, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 10816)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1024)              11076608  \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 11,125,578\n",
      "Trainable params: 11,125,578\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(lr=0.001), loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model checkpoint\n",
    "ModelCheckpoint helps us to save the model by monitoring a specific parameter of the model. In this case I am monitoring validation accuracy by passing val_acc to ModelCheckpoint. The model will only be saved to disk if the validation accuracy of the model in current epoch is greater than what it was in the last epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping\n",
    "EarlyStopping helps us to stop the training of the model early if there is no increase in the parameter which I have set to monitor in EarlyStopping. In this case I am monitoring validation accuracy by passing val_acc to EarlyStopping. I have here set patience to 20 which means that the model will stop to train if it doesnâ€™t see any rise in validation accuracy in 20 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch and batches\n",
    "\n",
    "In fit_generator steps_per_epoch will set the batch size to pass training data to the model and validation_steps will do the same for test data. You can tweak it based on your system specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"conv2d_1.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=2, verbose=1, mode='auto')\n",
    "hist = model.fit_generator(steps_per_epoch=1,generator=mnist.train, validation_data=mnist.test, validation_steps=10,epochs=30,callbacks=[checkpoint,early])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resulting model has a best accuracy score of 0.876"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16\n",
    "Let's examine the current best image recognition model, called VGG16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![vvg16](sources/vgg16.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and training VGG16 CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras,os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 images belonging to 2 classes.\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "trdata = ImageDataGenerator()\n",
    "traindata = trdata.flow_from_directory(directory=\"train\",target_size=(224,224))\n",
    "tsdata = ImageDataGenerator()\n",
    "testdata = tsdata.flow_from_directory(directory=\"test\", target_size=(224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_27 (Conv2D)           (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 8194      \n",
      "=================================================================\n",
      "Total params: 134,268,738\n",
      "Trainable params: 134,268,738\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=4096,activation=\"relu\"))\n",
    "model.add(Dense(units=4096,activation=\"relu\"))\n",
    "model.add(Dense(units=2, activation=\"softmax\"))\n",
    "opt = Adam(lr=0.001)\n",
    "model.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"vgg16_1.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=2, verbose=1, mode='auto')\n",
    "hist = model.fit_generator(steps_per_epoch=1,generator=traindata, validation_data= testdata, validation_steps=10,epochs=30,callbacks=[checkpoint,early])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above created model is a technical replica of VGG16.\n",
    "![vvg16](sources/vgg16_architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_27 (Conv2D)           (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 8194      \n",
      "=================================================================\n",
      "Total params: 134,268,738\n",
      "Trainable params: 134,268,738\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "opt = Adam(lr=0.001)\n",
    "model.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch and batches\n",
    "\n",
    "In fit_generator steps_per_epoch will set the batch size to pass training data to the model and validation_steps will do the same for test data. You can tweak it based on your system specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"vgg16_1.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=20, verbose=1, mode='auto')\n",
    "hist = model.fit_generator(steps_per_epoch=100,generator=traindata, validation_data= testdata, validation_steps=10,epochs=100,callbacks=[checkpoint,early])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "40 minutes per epoch with a total of 100 epochs? that's 4000 minutes, or about 67 hours of continuous training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "trdata = ImageDataGenerator()\n",
    "traindata = trdata.flow_from_directory(directory=\"cats_and_dogs_filtered/train\",target_size=(224,224))\n",
    "tsdata = ImageDataGenerator()\n",
    "testdata = tsdata.flow_from_directory(directory=\"cats_and_dogs_filtered/validation\", target_size=(224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "vggmodel = VGG16(weights='imagenet', include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x000001D3020F2588>\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001D37FDF1B00>\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001D302117748>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x000001D30212B6A0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001D30212B048>\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001D3021D72B0>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x000001D308E54CF8>\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001D308E40C88>\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001D308EFE198>\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001D308F2BBE0>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x000001D307D20EF0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001D307D20A20>\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001D307D584A8>\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001D307D6EF98>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x000001D307D85EB8>\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001D307D85C88>\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001D3020F2DD8>\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001D302153F98>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x000001D301A1D1D0>\n"
     ]
    }
   ],
   "source": [
    "for layers in (vggmodel.layers)[:19]:\n",
    "    print(layers)\n",
    "    layers.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 8194      \n",
      "=================================================================\n",
      "Total params: 134,268,738\n",
      "Trainable params: 119,554,050\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Krijn\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "X= vggmodel.layers[-2].output\n",
    "predictions = Dense(2, activation=\"softmax\")(X)\n",
    "model_final = Model(input = vggmodel.input, output = predictions)\n",
    "\n",
    "model_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])\n",
    "model_final.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 8194      \n",
      "=================================================================\n",
      "Total params: 134,268,738\n",
      "Trainable params: 119,554,050\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_final.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 30s 15s/step - loss: 1.3129 - acc: 0.5000 - val_loss: 1.0315 - val_acc: 0.4375\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.43750, saving model to vgg16_1.h5\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 28s 14s/step - loss: 0.6449 - acc: 0.7656 - val_loss: 0.5099 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.43750 to 0.75000, saving model to vgg16_1.h5\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 27s 14s/step - loss: 0.2599 - acc: 0.9219 - val_loss: 0.1164 - val_acc: 0.9688\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.75000 to 0.96875, saving model to vgg16_1.h5\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 27s 14s/step - loss: 0.2500 - acc: 0.8906 - val_loss: 0.1506 - val_acc: 0.9375\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.96875\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 28s 14s/step - loss: 0.3287 - acc: 0.8906 - val_loss: 0.1245 - val_acc: 0.9688\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.96875\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(\"vgg16_1.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=2, verbose=1, mode='auto')\n",
    "\n",
    "model_final.fit_generator(generator= traindata, steps_per_epoch= 2, epochs= 100, validation_data= testdata, validation_steps=1, callbacks=[checkpoint,early])\n",
    "model_final.save_weights(\"vgg16_1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge case model testing\n",
    "The model is trained to classify pictures of cats and dogs, and the performance of the model is measured with a 0.94 accuracy score. But what would happen with edge cases? e.g. entering different animals into the CNN. Lions are cats, however vastly different they might be. Dogs stem from the wolf, but does a CNN only trained on dogs think so too?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.08076258 0.91923743]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "image = load_img('wolf.jpg', target_size=(224, 224))\n",
    "# convert the image pixels to a numpy array\n",
    "image = img_to_array(image)\n",
    "# reshape data for the model\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "# prepare the image for the VGG model\n",
    "image = preprocess_input(image)\n",
    "# predict the probability across all output classes\n",
    "print(model_final.predict(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "https://arxiv.org/pdf/1707.09725.pdf#page=17\n",
    "\n",
    "https://d2l.ai/chapter_convolutional-neural-networks/padding-and-strides.html\n",
    "\n",
    "https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\n",
    "\n",
    "https://machinelearningmastery.com/introduction-to-1x1-convolutions-to-reduce-the-complexity-of-convolutional-neural-networks/\n",
    "\n",
    "https://medium.com/datadriveninvestor/notes-on-deep-learning-advanced-cnn-75ed499ca053\n",
    "\n",
    "https://engmrk.com/vgg16-implementation-using-keras\n",
    "\n",
    "https://towardsdatascience.com/neural-style-transfer-tutorial-part-1-f5cd3315fa7f\n",
    "\n",
    "https://deeplizard.com/learn/video/ZjM_XQa5s6s\n",
    "\n",
    "https://deeplizard.com/learn/video/YRhxdVk_sIs\n",
    "\n",
    "https://towardsdatascience.com/step-by-step-vgg16-implementation-in-keras-for-beginners-a833c686ae6c\n",
    "\n",
    "https://github.com/Ojaswy/Neural-Networks-Recognizing-hand-written-digits\n",
    "\n",
    "https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9\n",
    "\n",
    "https://gist.github.com/kashif/76792939dd6f473b7404474989cb62a8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
